{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8259d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "from pathlib import Path\n",
    "import json\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import time\n",
    "import gensim\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import re, string\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41bff43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load COCO metadata\n",
    "filename = \"data/captions_train2014.json\"\n",
    "with Path(filename).open() as f:\n",
    "    coco_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ad2ebc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with Path('data/resnet18_features.pkl').open('rb') as f:\n",
    "    resnet18_features = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4dbb556f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class coco_values:\n",
    "    def __init__(self, coco_data):\n",
    "        #make variables for images and captions\n",
    "        self.coco_annotations = coco_data['annotations']\n",
    "        self.coco_images = coco_data['images']\n",
    "        self.coco_img_ids = [i['id'] for i in self.coco_images if i['id'] in resnet18_features.keys()]\n",
    "        self.coco_cap_ids = [i['id'] for i in self.coco_annotations if i['image_id'] in resnet18_features.keys()]\n",
    "        self.image2cap = {}\n",
    "        for i in self.coco_annotations:\n",
    "            if i['image_id'] in self.image2cap.keys():\n",
    "                ids = self.image2cap[i['image_id']]\n",
    "                ids.append(i['id'])\n",
    "                self.image2cap[i['image_id']] = ids\n",
    "            else:\n",
    "                self.image2cap[i['image_id']] = [i['id']]\n",
    "        self.cap2image = {i['id']: i['image_id'] for i in self.coco_annotations}\n",
    "        self.cap2text = {i['id']: i['caption'] for i in self.coco_annotations}\n",
    "        self.image2url = {i['id']: i['coco_url'] for i in self.coco_images}\n",
    "        self.capID2embeded = None\n",
    "    def get_captions(self, image_id):\n",
    "        #Find the caption IDs by returning all caption IDs associated with an image ID\n",
    "        return self.image2cap[image_id]\n",
    "    def get_image_from_cap(self, caption_id):\n",
    "        #Find the image ID by seeing if any image had a caption ID match\n",
    "        return self.cap2image[caption_id]\n",
    "    def get_caption_text(self, caption_id):\n",
    "        #Take the text from a given caption ID\n",
    "        return self.cap2text[caption_id]\n",
    "    def get_url(self, image_id):\n",
    "        return self.image2url[image_id]\n",
    "    def init_id2vec(self, IDF):\n",
    "        self.capID2embeded = {}\n",
    "        punc_regex = re.compile('[{}]'.format(re.escape(string.punctuation)))\n",
    "        for i in self.coco_annotations:\n",
    "            given_str = punc_regex.sub('', i['caption'])\n",
    "            str_list = given_str.lower().split()\n",
    "            if i['id'] not in self.capID2embeded.keys():\n",
    "                    self.capID2embeded[i['id']] = []\n",
    "            for j in str_list:\n",
    "                if j in IDF:\n",
    "                    self.capID2embeded[i['id']] += IDF[j]\n",
    "                else:\n",
    "                    self.capID2embeded[i['id']].append(np.zeros(200))\n",
    "            \n",
    "    def compute_id2vec(self, caption_id):\n",
    "        if self.capID2embeded is None:\n",
    "            print(\"No caption to embeded dictionary created! Run init_id2vec before running\")\n",
    "            return None\n",
    "        return np.array(self.capID2embeded[caption_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6ebfefef",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = coco_values(coco_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "0797675d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[48, 126, 219, 255, 3555]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.get_captions(318556)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "4775a6f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "318556"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.get_image_from_cap(48)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "dc2e432c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A very clean and well decorated empty bathroom'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.get_caption_text(48)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "efcd1e16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 78.99814891815186s\n"
     ]
    }
   ],
   "source": [
    "#load glove dataset\n",
    "path = r\"data/glove.6B.200d.txt.w2v\"\n",
    "t0 = time.time()\n",
    "glove = KeyedVectors.load_word2vec_format(path, binary=False)\n",
    "t1 = time.time()\n",
    "print(\"elapsed %ss\" % (t1 - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5808d94e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (0,) (200,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_11068/1018273678.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minit_id2vec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mglove\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_11068/2649314921.py\u001b[0m in \u001b[0;36minit_id2vec\u001b[1;34m(self, IDF)\u001b[0m\n\u001b[0;32m     39\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mstr_list\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mIDF\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcapID2embeded\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'id'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mIDF\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcapID2embeded\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'id'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (0,) (200,) "
     ]
    }
   ],
   "source": [
    "x.init_id2vec(glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1d95df2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageToWord:\n",
    "    def __init__(self, context_words, d):\n",
    "        \"\"\" Initializes all of the encoder and decoder layers in our model, setting them\n",
    "        as attributes of the model.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        context_words : int\n",
    "            The number of context words included in our vocabulary\n",
    "            \n",
    "        d : int\n",
    "            The dimensionality of our word embeddings\n",
    "        \"\"\"\n",
    "        \n",
    "        # STUDENT CODE:\n",
    "        self.encode = dense(context_words, d, weight_initializer=glorot_normal, bias=False)\n",
    "        self.decode = dense(d, context_words, weight_initializer=glorot_normal, bias=False)\n",
    "        \n",
    "    \n",
    "    \n",
    "    def __call__(self, x):\n",
    "        ''' Passes data as input to our model, performing a \"forward-pass\".\n",
    "        \n",
    "        This allows us to conveniently initialize a model `m` and then send data through it\n",
    "        to be classified by calling `m(x)`.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        x : Union[numpy.ndarray, mygrad.Tensor], shape=(M, context_words)\n",
    "            A batch of data consisting of M words from the context matrix,\n",
    "                each tracking the number of co-occurences with `context_words` words.\n",
    "                \n",
    "        Returns\n",
    "        -------\n",
    "        mygrad.Tensor, shape=(M, context_words)\n",
    "            The result of passing the data through borth the encoder and decoder.\n",
    "        '''\n",
    "        \n",
    "        # STUDENT CODE:\n",
    "        return self.decode(self.encode(x))\n",
    "    \n",
    "    \n",
    "    @property\n",
    "    def parameters(self):\n",
    "        \"\"\" A convenience function for getting all the parameters of our model.\n",
    "        \n",
    "        This can be accessed as an attribute, via `model.parameters` \n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        Tuple[Tensor, ...]\n",
    "            A tuple containing all of the learnable parameters for our model\"\"\"\n",
    "        \n",
    "        # STUDENT CODE:\n",
    "        return self.encode.parameters + self.decode.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d722b504",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resnetID2vectors(N,resnet_model=None):\n",
    "    if resnet_model is None:\n",
    "        print(\"no resnet model given, using ResNet18\")\n",
    "        with Path('data/resnet18_features.pkl').open('rb') as f:\n",
    "            resnet_model = pickle.load(f)\n",
    "    return [resnet_model[i] for i in N if i in resnet_model.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1d2d8ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#resnetID2vectors([318556])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c09aa908",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(image_ids, coco, num_good_captions: int, triples_per_caption: int = 10,tournment_size=25):\n",
    "    triples = []\n",
    "    for i in range(triples_per_caption):\n",
    "        #getting good image\n",
    "        good_image_id = random.choice(image_ids)\n",
    "        \n",
    "        #getting caption\n",
    "        good_caption_id: int = random.choice(coco.get_captions(good_image_id))\n",
    "        good_caption_embedding = coco.compute_id2vec(good_caption_id)\n",
    "        \n",
    "        #add triples (pairs for now)\n",
    "        triples.append((good_caption_id, good_image_id))\n",
    "        \n",
    "    np.random.shuffle(triples)\n",
    "    \n",
    "    return triples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ec794c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_dist(di, dj):\n",
    "    dot_prod = np.vdot(di, dj)\n",
    "    denom = sum(i**2 for i in di.T)**0.5 * sum(j**2 for j in dj.T)**0.5\n",
    "    cosine_dist = 1 - dot_prod/denom\n",
    "    return float(cosine_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "e95758a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def caption2badimage(caption_embedding, good_image_id, possible_im_ids, coco, tournament_size):\n",
    "    best_score = -1.0\n",
    "    best_bad_img_id = -1\n",
    "    for _ in range(tournament_size):\n",
    "        bad_image_id = good_image_id\n",
    "        while bad_image_id == good_image_id:\n",
    "            bad_image_id = np.random.choice(possible_im_ids)\n",
    "        \n",
    "        bad_caption_id = np.random.choice(coco.get_captions(bad_image_id))\n",
    "        \n",
    "        bad_embedding = coco.compute_id2vec(bad_caption_id)\n",
    "        #bad_embedding = np.reshape(bad_embedding,(bad_embedding.shape[1],bad_embedding.shape[0]))\n",
    "        print(caption_embedding.shape)\n",
    "        print(bad_embedding.shape\n",
    "        sim = np.matmul(caption_embedding,bad_embedding)\n",
    "        \n",
    "        if sim > best_score:\n",
    "            best_score = sim\n",
    "            best_bad_img_id = bad_image_id        \n",
    "    return best_bad_img_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "767c3120",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 200)\n",
      "(200, 12)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_12896/983955996.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mID\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m48\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mcaption2badimage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_id2vec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mID\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_image_from_cap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mID\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcoco_img_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_12896/94855830.py\u001b[0m in \u001b[0;36mcaption2badimage\u001b[1;34m(caption_embedding, good_image_id, possible_im_ids, coco, tournament_size)\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0msim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcaption_embedding\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbad_embedding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0msim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mbest_score\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m             \u001b[0mbest_score\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msim\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m             \u001b[0mbest_bad_img_id\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbad_image_id\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    }
   ],
   "source": [
    "ID = 48\n",
    "caption2badimage(x.compute_id2vec(ID), x.get_image_from_cap(ID),x.coco_img_ids, x,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b80110d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
